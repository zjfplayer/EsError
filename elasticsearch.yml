
# ======================== Elasticsearch Configuration =========================
#
# NOTE: Elasticsearch comes with reasonable defaults for most settings.
#       Before you set out to tweak and tune the configuration, make sure you
#       understand what are you trying to accomplish and the consequences.
#
# The primary way of configuring a node is via this file. This template lists
# the most important settings you may want to configure for a production cluster.
#
# Please see the documentation for further information on configuration options:
# <http://www.elastic.co/guide/en/elasticsearch/reference/current/setup-configuration.html>
#
# ---------------------------------- Cluster -----------------------------------
#
# Use a descriptive name for your cluster:
#
cluster.name: cluster
#
# ------------------------------------ Node ------------------------------------
#
# Use a descriptive name for the node:
#
node.name: tdh-3_instancegroup1
#
# Add custom attributes to the node:
#
# node.attr.rack: r1
#
# Every node can be configured to allow or deny being eligible as the master,
# and to allow or deny to store the data.
#
# Allow this node to be eligible as a master node (enabled by default):
#
node.master: true
#
# Allow this node to store data (enabled by default):
#
node.data: true
#
# ----------------------------------- Paths ------------------------------------
#
# Path to directory where to store the data (separate multiple locations by comma):
#
path.data: /vdir/mnt/disk1/search/instancegroup1/data,/vdir/mnt/disk2/search/instancegroup1/data
#
# Path to log files:
#
path.logs: /var/log/search1/instancegroup1
#
# ----------------------------------- Memory -----------------------------------
#
# Lock the memory on startup:
#
# bootstrap.mlockall: true
#
# Make sure that the `ES_HEAP_SIZE` environment variable is set to about half the memory
# available on the system and that the owner of the process is allowed to use this limit.
#
# Elasticsearch performs poorly when the system is swapping the memory.
#
# ---------------------------------- Network -----------------------------------
#
# Set the bind adress to a specific IP (IPv4 or IPv6):
#
network.bind_host: 0.0.0.0
network.publish_host: tdh-3
#
# Set a custom port for HTTP:
#
http.port: 9201
transport.tcp.port: 9301
#
# For more information, see the documentation at:
# <http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-network.html>
#
# ---------------------------------- Gateway -----------------------------------
#
# Block initial recovery after a full cluster restart until N nodes are started:
#
gateway.recover_after_time: 5m
gateway.recover_after_nodes: 2
gateway.recover_after_data_nodes: 2
gateway.recover_after_master_nodes: 2
gateway.expected_nodes: 3
gateway.expected_data_nodes: 3
gateway.expected_master_nodes: 3
#
# For more information, see the documentation at:
# <http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-gateway.html>
#
# --------------------------------- Discovery ----------------------------------
#
# Elasticsearch nodes will find each other via unicast, by default.
#
# Pass an initial list of hosts to perform discovery when new node is started:
# The default list of hosts is ["127.0.0.1", "[::1]"]
#
discovery.zen.ping.unicast.hosts: tdh-2:9301,tdh-1:9301,tdh-3:9301
#
# Prevent the "split brain" by configuring the majority of nodes (total number of nodes / 2 + 1):
#
discovery.zen.minimum_master_nodes: 2
#
# For more information, see the documentation at:
# <http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-discovery.html>
#
# ---------------------------------- Various -----------------------------------
#
# Disable starting multiple nodes on a single system:
#
# node.max_local_storage_nodes: 1
#
# Require explicit names when deleting indices:
#
# action.destructive_requires_name: true
#
# ---------------------------------- Thread Pool -------------------------------
#
# For more information, see the documentation at:
# <https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-threadpool.html>
#
thread_pool:
  bulk:
    queue_size: 5000

license.zookeeper.quorum: tdh-2:2291,tdh-1:2291,tdh-3:2291
index.shard.path.selector: tiered
bootstrap.system_call_filter: false
http.cors.enabled: true
http.cors.allow-origin: "*"
indices.fielddata.cache.size: 50%
index.store.type: niofs
index.translog.durability: async

discovery.zen.fd.ping_timeout: 120s
discovery.zen.fd.ping_retries: 6
discovery.zen.fd.ping_interval: 30s
discovery.zen.ping_timeout: 100s
cluster.routing.allocation.balance.shard: 0.001
cluster.routing.allocation.balance.index: 0.999
index.unassigned.node_left.delayed_timeout: 10m
cluster.routing.allocation.same_shard.host: true

# slow log
index.indexing.slowlog.level: info
index.indexing.slowlog.threshold.index.debug: 2s
index.indexing.slowlog.threshold.index.info: 5s
index.indexing.slowlog.threshold.index.trace: 500ms
index.indexing.slowlog.threshold.index.warn: 10s
index.search.slowlog.threshold.fetch.debug: 500ms
index.search.slowlog.threshold.fetch.info: 800ms
index.search.slowlog.threshold.fetch.trace: 200ms
index.search.slowlog.threshold.fetch.warn: 1s
index.search.slowlog.threshold.query.debug: 2s
index.search.slowlog.threshold.query.info: 5s
index.search.slowlog.threshold.query.trace: 500ms
index.search.slowlog.threshold.query.warn: 10s

index.number_of_shards: 10
index.number_of_replicas: 1

transport.type: netty3
http.type: netty3

action.auto_create_index: +logs-*, +persisted-*, -*

